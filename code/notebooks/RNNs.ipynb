{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from nov20.prepare_data import parse_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "summaries_dir = 'nov20/summaries/shakespeare_{}'.format(seq_len)\n",
    "prefix = 'nov20/shakespeare/seq{}'.format(seq_len)\n",
    "seq_file = prefix + '.tfrecords'\n",
    "vocab_file = prefix + '_vocab'\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "\n",
    "with open(vocab_file, 'rb') as fin:\n",
    "    ch_to_idx = pickle.load(fin)\n",
    "    num_chars = len(ch_to_idx)    \n",
    "\n",
    "dataset = tf.contrib.data.TFRecordDataset([seq_file])\n",
    "dataset = dataset.map(lambda x: parse_seq(x, seq_len=seq_len))\n",
    "dataset = dataset.map(lambda x: tf.one_hot(x, num_chars))\n",
    "dataset = dataset.shuffle(1000).batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_size = 512\n",
    "\n",
    "init_state = tf.placeholder(tf.float32, [None, hidden_state_size])\n",
    "hidden_state = init_state\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    x_seq = iterator.get_next()\n",
    "    \n",
    "with tf.variable_scope(\"hidden\"):\n",
    "    W_xh = tf.get_variable('W_xh', [num_chars, hidden_state_size])\n",
    "    B_xh = tf.get_variable('B_xh', [hidden_state_size])\n",
    "\n",
    "    W_hh = tf.get_variable('W_hh', [hidden_state_size, hidden_state_size])\n",
    "    B_hh = tf.get_variable('B_hh', [1, hidden_state_size])\n",
    "\n",
    "with tf.variable_scope(\"output\"):\n",
    "    W_hy = tf.get_variable('W_hy', [hidden_state_size, num_chars])\n",
    "    B_hy = tf.get_variable('B_hy', [num_chars])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unroll computation graph\n",
    "costs = []\n",
    "for i in range(seq_len - 1):\n",
    "    x = x_seq[:, i, :]\n",
    "    y_ = x_seq[:, i+1, :]\n",
    "\n",
    "    hidden_state = tf.nn.tanh(tf.matmul(x, W_xh) + tf.matmul(hidden_state, W_hh) + B_hh)\n",
    "    y = tf.matmul(hidden_state, W_hy) + B_hy\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=y_, logits=y))\n",
    "    costs.append(cross_entropy)\n",
    "\n",
    "total_cost = tf.reduce_mean(costs)\n",
    "train_op = tf.train.AdamOptimizer().minimize(total_cost)\n",
    "\n",
    "tf.summary.scalar('total_cost', total_cost)\n",
    "summary_op = tf.summary.merge_all()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\tcost 3.8123011589050293\n",
      "Step 500\tcost 2.015751361846924\n",
      "Step 1000\tcost 1.780280351638794\n",
      "Step 1500\tcost 1.716383695602417\n",
      "Step 2000\tcost 1.5858182907104492\n",
      "Step 2500\tcost 1.5415319204330444\n",
      "Step 3000\tcost 1.4874902963638306\n",
      "Step 3500\tcost 1.4646861553192139\n",
      "Step 4000\tcost 1.4239393472671509\n",
      "Step 4500\tcost 1.3797961473464966\n",
      "Step 5000\tcost 1.4206748008728027\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sesh:\n",
    "    writer = tf.summary.FileWriter(summaries_dir, sesh.graph)\n",
    "    sesh.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(5000+1):\n",
    "        _ = sesh.run([train_op],\n",
    "                     feed_dict={init_state: np.zeros((1, hidden_state_size))})\n",
    "        if step % 500 == 0:\n",
    "            xent, summary, _ = sesh.run([total_cost, summary_op, train_op],\n",
    "                     feed_dict={init_state: np.zeros((1, hidden_state_size))})\n",
    "            writer.add_summary(summary, step)\n",
    "            print(\"Step {}\\tcost {}\".format(step, xent))\n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sesh, summaries_dir + \"/model.ckpt\", global_step=step)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 200\n",
    "summaries_dir = 'nov20/summaries/shakespeare_{}'.format(seq_len)\n",
    "prefix = 'nov20/shakespeare/seq{}'.format(seq_len)\n",
    "seq_file = prefix + '.tfrecords'\n",
    "vocab_file = prefix + '_vocab'\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "\n",
    "with open(vocab_file, 'rb') as fin:\n",
    "    ch_to_idx = pickle.load(fin)\n",
    "    num_chars = len(ch_to_idx)\n",
    "idx_to_ch = {v: k for k, v in ch_to_idx.items()}\n",
    "    \n",
    "hidden_state_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from nov20/summaries/shakespeare_200/model.ckpt-5000\n",
      "ase of Caccas'd, bring to\n",
      "    son the flooga; some dis, is villain, I wist me now enteracains, I will grant.-\n",
      "                 Enter CLERES OT SBRAKE, the ELIUS and BYYBOG HOMPERLE, and JOHT Canddoom, Thy is friends\n",
      "    That promisad homour-he drawion; the plivit out, foul man, but most fault,\n",
      "    Do more true; I was at the cursenur, till his good narumenty hath, was true? serewings the fainted knave,\n",
      "    I wese make real and preventinent, as so not be nong.\n",
      "\n",
      "            which, we werc's me call\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model_path = summaries_dir + \"/model.ckpt-5000\"\n",
    "\n",
    "input_char = tf.placeholder(tf.float32, [None, num_chars])\n",
    "input_state = tf.placeholder(tf.float32, [None, hidden_state_size])\n",
    "hidden_state = input_state\n",
    "    \n",
    "with tf.variable_scope(\"hidden\"):\n",
    "    W_xh = tf.get_variable('W_xh', [num_chars, hidden_state_size])\n",
    "    B_xh = tf.get_variable('B_xh', [hidden_state_size])\n",
    "\n",
    "    W_hh = tf.get_variable('W_hh', [hidden_state_size, hidden_state_size])\n",
    "    B_hh = tf.get_variable('B_hh', [1, hidden_state_size])\n",
    "\n",
    "with tf.variable_scope(\"output\"):\n",
    "    W_hy = tf.get_variable('W_hy', [hidden_state_size, num_chars])\n",
    "    B_hy = tf.get_variable('B_hy', [num_chars])\n",
    "    \n",
    "\n",
    "hidden_state = tf.nn.tanh(tf.matmul(input_char, W_xh) + tf.matmul(hidden_state, W_hh) + B_hh)\n",
    "y = tf.matmul(hidden_state, W_hy) + B_hy\n",
    "\n",
    "probs_op = tf.nn.softmax(logits=y)\n",
    "\n",
    "output = ''\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sesh:\n",
    "    saver.restore(sesh, model_path)\n",
    "    cur_char = np.zeros((1, num_chars))\n",
    "    cur_char[0, 0] = 1.\n",
    "    \n",
    "    cur_state = np.zeros((1, hidden_state_size))\n",
    "    \n",
    "    for _ in range(500):\n",
    "        probs, next_state = sesh.run([probs_op, hidden_state], feed_dict={\n",
    "            input_char: cur_char,\n",
    "            input_state: cur_state\n",
    "        })\n",
    "        probs = np.squeeze(probs)\n",
    "        next_char_pos = np.random.choice(num_chars, p=probs)\n",
    "        output +=  idx_to_ch[next_char_pos]\n",
    "        \n",
    "        cur_char = np.zeros((1, num_chars))\n",
    "        cur_char[0, next_char_pos] = 1.\n",
    "        cur_state = next_state\n",
    "\n",
    "print(output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
