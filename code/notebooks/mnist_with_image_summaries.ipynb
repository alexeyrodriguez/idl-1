{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_var(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_var(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, strides=[1, 1, 1, 1]):\n",
    "    return tf.nn.conv2d(x, W, strides=strides, padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# build computation graph\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "kernel_size = [19, 19]\n",
    "\n",
    "with tf.name_scope('conv1'):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=x_img, filters=32, kernel_size=kernel_size,\n",
    "        padding='same', activation=tf.nn.relu,\n",
    "        name='conv1'\n",
    "    )\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2, 2), strides=2)\n",
    "\n",
    "# save images\n",
    "conv1_filters = [var for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')\n",
    "           if 'kernel' in var.name][0]\n",
    "tf.summary.image('conv1_weights', tf.transpose(conv1_filters, (3, 0, 1, 2)), max_outputs=32)\n",
    "\n",
    "\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1, filters=64, kernel_size=kernel_size,\n",
    "    padding='same', activation=tf.nn.relu\n",
    ")\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2, 2), strides=2)\n",
    "\n",
    "pool2flat = tf.reshape(pool2, [-1, 7*7*64])\n",
    "dense = tf.layers.dense(inputs=pool2flat, units=1024, activation=tf.nn.relu)\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=0.5, training=is_training)\n",
    "\n",
    "y = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding run metadata for 99\n",
      "Adding run metadata for 199\n",
      "Adding run metadata for 299\n",
      "Adding run metadata for 399\n",
      "Adding run metadata for 499\n",
      "Adding run metadata for 599\n",
      "Adding run metadata for 699\n",
      "Adding run metadata for 799\n",
      "Adding run metadata for 899\n",
      "Adding run metadata for 999\n",
      "Adding run metadata for 1099\n",
      "Adding run metadata for 1199\n",
      "Adding run metadata for 1299\n",
      "Adding run metadata for 1399\n",
      "Adding run metadata for 1499\n",
      "Adding run metadata for 1599\n",
      "Adding run metadata for 1699\n",
      "Adding run metadata for 1799\n",
      "Adding run metadata for 1899\n",
      "Adding run metadata for 1999\n",
      "CPU times: user 2min 32s, sys: 15.4 s, total: 2min 47s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "summaries_dir = 'tb/run6-19'\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sesh:\n",
    "    train_writer = tf.summary.FileWriter(summaries_dir + '/train', sesh.graph)\n",
    "    test_writer = tf.summary.FileWriter(summaries_dir + '/test')\n",
    "    sesh.run(tf.global_variables_initializer())\n",
    "    batch_size = 25\n",
    "    \n",
    "    test_xs, test_ys = mnist.test.images, mnist.test.labels\n",
    "    \n",
    "    for i in range(2000):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        if i % 10 == 0: # record test set accuracy\n",
    "            summary, acc = sesh.run([merged, accuracy], feed_dict={x: test_xs, y_: test_ys, is_training: False})\n",
    "            test_writer.add_summary(summary, i)\n",
    "        else:\n",
    "            if i % 100 == 99: # record train set accuracy\n",
    "                summary, _ = sesh.run([merged, train_step], \n",
    "                                      feed_dict={x: batch[0],\n",
    "                                                 y_: batch[1],\n",
    "                                                 is_training: True},\n",
    "                                      )\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  # Record a summary\n",
    "                summary, _ = sesh.run([merged, train_step], feed_dict={x: batch[0],\n",
    "                                                 y_: batch[1],\n",
    "                                                 is_training: True})\n",
    "                train_writer.add_summary(summary, i)\n",
    "                \n",
    "test_writer.close()\n",
    "train_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
