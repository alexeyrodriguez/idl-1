{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image\n",
    "\n",
    "from datasets import MNISTDataset\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy on the 0th step: 0.140625\n",
      "Training accuracy on the 100th step: 0.828125\n",
      "Training accuracy on the 200th step: 0.88671875\n",
      "Starting new epoch...\n",
      "Training accuracy on the 300th step: 0.84765625\n",
      "Training accuracy on the 400th step: 0.9453125\n",
      "Starting new epoch...\n",
      "Training accuracy on the 500th step: 0.97265625\n",
      "Training accuracy on the 600th step: 0.96875\n",
      "Training accuracy on the 700th step: 0.95703125\n",
      "Starting new epoch...\n",
      "Training accuracy on the 800th step: 0.97265625\n",
      "Training accuracy on the 900th step: 0.98828125\n",
      "Starting new epoch...\n",
      "Final test accuracy: [0.94750017, 0.17990339]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tb_dir = 'summaries/itworks'\n",
    "\n",
    "# get the data\n",
    "mnist = MNISTDataset(\"mnist_data\", batch_size=256, seed=int(time()))\n",
    "\n",
    "# define the model first, from input to output\n",
    "imgs = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "\n",
    "n_hidden = 100\n",
    "n_layers = 8\n",
    "w_range = 0.1\n",
    "hidden = imgs\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    name = \"hidden_{}\".format(layer)\n",
    "    hidden = tf.layers.dense(hidden, n_hidden, activation=tf.nn.relu,\n",
    "                             #kernel_initializer=tf.random_uniform_initializer(minval=-w_range, maxval=w_range),\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "                                 uniform=True),\n",
    "                             bias_initializer=tf.constant_initializer(0.01), name=name)\n",
    "    tf.summary.histogram(name + \"_hist\", hidden)\n",
    "logits = tf.layers.dense(hidden, 10, kernel_initializer=tf.random_uniform_initializer(minval=-w_range, maxval=w_range),\n",
    "                         bias_initializer=tf.zeros_initializer, name=\"logits\")\n",
    "\n",
    "\n",
    "# create the cost and a single training step based on that\n",
    "labels = tf.placeholder(tf.uint8, [None])\n",
    "labels_onehot = tf.one_hot(indices=labels, depth=10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits))\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.03)\n",
    "for g, v in optimizer.compute_gradients(cross_entropy):\n",
    "    if 'kernel' in v.name:\n",
    "        tf.summary.scalar(\"grad_{}\".format(v.name.replace(':','_')), tf.norm(g))\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# evaluation measures\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# train!\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tb_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.next_batch()\n",
    "        stats, _ = sess.run([merged, train_step], feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "        writer.add_summary(stats, step)\n",
    "        if step % 100 == 0:\n",
    "            train_acc, _ = sess.run([accuracy, cross_entropy], feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "            print(\"Training accuracy on the {}th step: {}\".format(step, train_acc))\n",
    "\n",
    "    # now evaluate\n",
    "    print(\"Final test accuracy: {}\".format(sess.run([accuracy, cross_entropy],\n",
    "                                                    feed_dict={imgs: mnist.test_data, labels: mnist.test_labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_0/kernel:0 is illegal; using grad_hidden_0/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_0/kernel:0 is illegal; using grad_hidden_0/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_0/bias:0 is illegal; using grad_hidden_0/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_0/bias:0 is illegal; using grad_hidden_0/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_1/kernel:0 is illegal; using grad_hidden_1/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_1/kernel:0 is illegal; using grad_hidden_1/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_1/bias:0 is illegal; using grad_hidden_1/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_1/bias:0 is illegal; using grad_hidden_1/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_2/kernel:0 is illegal; using grad_hidden_2/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_2/kernel:0 is illegal; using grad_hidden_2/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_2/bias:0 is illegal; using grad_hidden_2/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_2/bias:0 is illegal; using grad_hidden_2/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_3/kernel:0 is illegal; using grad_hidden_3/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_3/kernel:0 is illegal; using grad_hidden_3/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_3/bias:0 is illegal; using grad_hidden_3/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_3/bias:0 is illegal; using grad_hidden_3/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_4/kernel:0 is illegal; using grad_hidden_4/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_4/kernel:0 is illegal; using grad_hidden_4/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_4/bias:0 is illegal; using grad_hidden_4/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_4/bias:0 is illegal; using grad_hidden_4/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_5/kernel:0 is illegal; using grad_hidden_5/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_5/kernel:0 is illegal; using grad_hidden_5/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_5/bias:0 is illegal; using grad_hidden_5/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_5/bias:0 is illegal; using grad_hidden_5/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_6/kernel:0 is illegal; using grad_hidden_6/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_6/kernel:0 is illegal; using grad_hidden_6/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_6/bias:0 is illegal; using grad_hidden_6/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_6/bias:0 is illegal; using grad_hidden_6/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_7/kernel:0 is illegal; using grad_hidden_7/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_7/kernel:0 is illegal; using grad_hidden_7/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_7/bias:0 is illegal; using grad_hidden_7/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_hidden_7/bias:0 is illegal; using grad_hidden_7/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_logits/kernel:0 is illegal; using grad_logits/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_logits/kernel:0 is illegal; using grad_logits/kernel_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_logits/bias:0 is illegal; using grad_logits/bias_0 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name grad_logits/bias:0 is illegal; using grad_logits/bias_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy on the 0th step: [0.13671875, 182.54266]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Infinity in summary histogram for: hidden3/hidden_3\n\t [[Node: hidden3/hidden_3 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](hidden3/hidden_3/tag, hidden_3/Relu/_73)]]\n\nCaused by op 'hidden3/hidden_3', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-64-7cb14f620324>\", line 22, in <module>\n    tf.summary.histogram(name, hidden)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/summary/summary.py\", line 192, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 129, in _histogram_summary\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Infinity in summary histogram for: hidden3/hidden_3\n\t [[Node: hidden3/hidden_3 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](hidden3/hidden_3/tag, hidden_3/Relu/_73)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Infinity in summary histogram for: hidden3/hidden_3\n\t [[Node: hidden3/hidden_3 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](hidden3/hidden_3/tag, hidden_3/Relu/_73)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-7cb14f620324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Infinity in summary histogram for: hidden3/hidden_3\n\t [[Node: hidden3/hidden_3 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](hidden3/hidden_3/tag, hidden_3/Relu/_73)]]\n\nCaused by op 'hidden3/hidden_3', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-64-7cb14f620324>\", line 22, in <module>\n    tf.summary.histogram(name, hidden)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/summary/summary.py\", line 192, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 129, in _histogram_summary\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Infinity in summary histogram for: hidden3/hidden_3\n\t [[Node: hidden3/hidden_3 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](hidden3/hidden_3/tag, hidden_3/Relu/_73)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "tb_dir = 'summaries/ex1'\n",
    "\n",
    "# get the data\n",
    "mnist = MNISTDataset(\"mnist_data\", batch_size=256, seed=int(time()))\n",
    "\n",
    "\n",
    "# define the model first, from input to output\n",
    "imgs = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "\n",
    "n_hidden = 100\n",
    "n_layers = 8\n",
    "w_range = 0.15\n",
    "hidden = imgs\n",
    "for layer in range(n_layers):\n",
    "    name = 'hidden_{}'.format(layer)\n",
    "    hidden = tf.layers.dense(\n",
    "            hidden, n_hidden, activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_uniform_initializer(minval=-w_range, maxval=w_range),\n",
    "            bias_initializer=tf.constant_initializer(0.01), name=name)\n",
    "    with tf.name_scope('hidden'+str(layer)) as scope:\n",
    "        tf.summary.histogram(name, hidden)\n",
    "\n",
    "logits = tf.layers.dense(hidden, 10,\n",
    "        kernel_initializer=tf.random_uniform_initializer(minval=-w_range, maxval=w_range),\n",
    "        bias_initializer=tf.zeros_initializer, name=\"logits\")\n",
    "\n",
    "\n",
    "# create the cost and a single training step based on that\n",
    "labels = tf.placeholder(tf.uint8, [None])\n",
    "labels_onehot = tf.one_hot(indices=labels, depth=10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits))\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "for g, v in optimizer.compute_gradients(cross_entropy):\n",
    "    tf.summary.scalar(\"grad_{}\".format(v.name.replace(':','_')), tf.norm(g))\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# evaluation measures\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# train!\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tb_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.next_batch()\n",
    "        stats, _ = sess.run([merged, train_step], feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "        writer.add_summary(stats, step)\n",
    "        if step % 100 == 0:\n",
    "            train_acc = sess.run([accuracy, cross_entropy], feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "            print(\"Training accuracy on the {}th step: {}\".format(step, train_acc))\n",
    "\n",
    "    # now evaluate\n",
    "    print(\"Final test accuracy: {}\".format(sess.run([accuracy, cross_entropy],\n",
    "                                                    feed_dict={imgs: mnist.test_data, labels: mnist.test_labels})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tb_dir = 'summaries/ex2'\n",
    "\n",
    "# get the data\n",
    "mnist = MNISTDataset(\"mnist_data\", batch_size=256, seed=int(time()))\n",
    "\n",
    "# define the model first, from input to output\n",
    "imgs = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "\n",
    "n_hidden = 100\n",
    "n_layers = 8\n",
    "w_range = 0.1\n",
    "hidden = imgs\n",
    "for layer in range(n_layers):\n",
    "    name = \"hidden_{}\".format(layer)\n",
    "    with tf.name_scope(name) as scope:\n",
    "        hidden = tf.layers.dense(hidden, n_hidden, activation=tf.nn.sigmoid,\n",
    "                                 kernel_initializer=tf.random_uniform_initializer(minval=-w_range, maxval=w_range),\n",
    "                                 bias_initializer=tf.zeros_initializer, name=name)\n",
    "        tf.summary.histogram(\"histogram\", hidden)\n",
    "logits = tf.layers.dense(hidden, 10, kernel_initializer=tf.random_uniform_initializer(minval=-w_range, maxval=w_range),\n",
    "                         bias_initializer=tf.zeros_initializer, name=\"logits\")\n",
    "\n",
    "\n",
    "# create the cost and a single training step based on that\n",
    "labels = tf.placeholder(tf.uint8, [None])\n",
    "labels_onehot = tf.one_hot(indices=labels, depth=10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits))\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.03)\n",
    "for g, v in optimizer.compute_gradients(cross_entropy):\n",
    "    if 'kernel' in v.name:\n",
    "        tf.summary.scalar(\"grad_{}\".format(v.name), tf.norm(g))\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# evaluation measures\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# train!\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tb_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.next_batch()\n",
    "        stats, _ = sess.run([merged, train_step], feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "        writer.add_summary(stats, step)\n",
    "        if step % 100 == 0:\n",
    "            train_acc = sess.run([accuracy, cross_entropy], feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "            print(\"Training accuracy on the {}th step: {}\".format(step, train_acc))\n",
    "\n",
    "    # now evaluate\n",
    "    print(\"Final test accuracy: {}\".format(sess.run([accuracy, cross_entropy],\n",
    "                                                    feed_dict={imgs: mnist.test_data, labels: mnist.test_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(x):\n",
    "    return np.power(1. + np.exp(-x), -1.)\n",
    "\n",
    "def ds(x):\n",
    "    return s(x)*(1.-s(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH4wYBerEQT08AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a8732e3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.linspace(-5, 5, 200)\n",
    "plt.plot(xs, s(xs))\n",
    "# plt.plot(xs, ds(xs), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy on the 0th step: [0.1328125, 2.3024108]\n",
      "Training accuracy on the 100th step: [0.14453125, 2.2994375]\n",
      "Training accuracy on the 200th step: [0.09765625, 2.3022189]\n",
      "Starting new epoch...\n",
      "Training accuracy on the 300th step: [0.10546875, 2.3013077]\n",
      "Training accuracy on the 400th step: [0.12109375, 2.2966061]\n",
      "Starting new epoch...\n",
      "Training accuracy on the 500th step: [0.12890625, 2.3025215]\n",
      "Training accuracy on the 600th step: [0.12890625, 2.2965834]\n",
      "Training accuracy on the 700th step: [0.1015625, 2.3049922]\n",
      "Starting new epoch...\n",
      "Training accuracy on the 800th step: [0.1171875, 2.2996244]\n",
      "Training accuracy on the 900th step: [0.0703125, 2.3059671]\n",
      "Starting new epoch...\n",
      "Final test accuracy: [0.11350001, 2.3010006]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tb_dir = 'summaries/ex3'\n",
    "\n",
    "# get the data\n",
    "mnist = MNISTDataset(\"mnist_data\", batch_size=256, seed=int(time()))\n",
    "\n",
    "\n",
    "# define the model first, from input to output\n",
    "imgs = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "\n",
    "hidden1 = tf.layers.dense(imgs, 100, activation=tf.nn.relu,\n",
    "                          kernel_initializer=tf.random_uniform_initializer(minval=-0.01, maxval=0.01),\n",
    "                          bias_initializer=tf.constant_initializer(0.01), name=\"hidden1\")\n",
    "hidden2 = tf.layers.dense(hidden1, 100, activation=tf.nn.relu,\n",
    "                          kernel_initializer=tf.random_uniform_initializer(minval=-0.01, maxval=0.00),\n",
    "                          bias_initializer=tf.constant_initializer(0.01), name=\"hidden2\")\n",
    "\n",
    "\n",
    "logits = tf.layers.dense(hidden2, 10, kernel_initializer=tf.random_uniform_initializer(minval=-0.01, maxval=0.01),\n",
    "                         bias_initializer=tf.zeros_initializer, name=\"logits\")\n",
    "\n",
    "tf.summary.histogram('hidden1', hidden1)\n",
    "tf.summary.histogram('hidden2', hidden2)\n",
    "\n",
    "# create the cost and a single training step based on that\n",
    "labels = tf.placeholder(tf.uint8, [None])\n",
    "labels_onehot = tf.one_hot(indices=labels, depth=10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits))\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "for g, v in optimizer.compute_gradients(cross_entropy):\n",
    "    if 'kernel' in v.name:\n",
    "        tf.summary.scalar(\"grad_{}\".format(v.name.replace(':','_')), tf.norm(g))\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# evaluation measures\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# train!\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tb_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.next_batch()\n",
    "        stats, _ = sess.run([merged, train_step], feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "        writer.add_summary(stats, step)\n",
    "        if step % 100 == 0:\n",
    "            train_acc = sess.run([accuracy, cross_entropy], feed_dict={imgs: batch_xs, labels: batch_ys})\n",
    "            print(\"Training accuracy on the {}th step: {}\".format(step, train_acc))\n",
    "\n",
    "    # now evaluate\n",
    "    print(\"Final test accuracy: {}\".format(sess.run([accuracy, cross_entropy],\n",
    "                                                    feed_dict={imgs: mnist.test_data, labels: mnist.test_labels})))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sprite(imgs, out_path):\n",
    "    \"\"\"\n",
    "    imgs: 2D (!) numpy array containing MNIST image data (i.e. 784-length vectors).\n",
    "    out_path: Path to store sprite image at (e.g. path/to/file.png).\n",
    "    \"\"\"\n",
    "    imgs_square = imgs.reshape(-1, 28, 28)\n",
    "    grid_dim = int(np.ceil(np.sqrt(len(imgs))))\n",
    "    grid_rows = [np.concatenate(imgs_square[ind:(ind+grid_dim)], axis=1) for ind in range(0, len(imgs), grid_dim)]\n",
    "    # maybe pad last row\n",
    "    missing = grid_rows[0].shape[1] - grid_rows[-1].shape[1]\n",
    "    if missing:\n",
    "        grid_rows[-1] = np.pad(grid_rows[-1], ((0, 0), (0, missing)), mode=\"constant\")\n",
    "    grid = np.concatenate(grid_rows, axis=0)\n",
    "    # note that the images are reversed (black on white background) for readability\n",
    "    image.imsave(out_path, grid, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training accuracy on the 0th step: 0.1159999892115593\n",
      "Training accuracy on the 100th step: 0.12999999523162842\n",
      "Training accuracy on the 200th step: 0.2369999885559082\n",
      "Training accuracy on the 300th step: 0.4490000009536743\n",
      "Training accuracy on the 400th step: 0.7330000400543213\n",
      "Training accuracy on the 500th step: 0.8340000510215759\n",
      "Training accuracy on the 600th step: 0.843999981880188\n",
      "Training accuracy on the 700th step: 0.9020000100135803\n",
      "Training accuracy on the 800th step: 0.8969999551773071\n",
      "Training accuracy on the 900th step: 0.9119999408721924\n",
      "Final test accuracy: [0.92090011, 0.25431779]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tb_dir = 'summaries/embeddings_orig'\n",
    "\n",
    "# get the data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# define the model first, from input to output\n",
    "imgs = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "\n",
    "\n",
    "n_hidden = 100\n",
    "n_layers = 8\n",
    "w_range = 0.1\n",
    "hidden = imgs\n",
    "num_to_viz = 256\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    name = \"hidden_{}\".format(layer)\n",
    "    hidden = tf.layers.dense(hidden, n_hidden, activation=tf.nn.relu,\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "                                 uniform=True),\n",
    "                             bias_initializer=tf.constant_initializer(0.01), name=name)\n",
    "    tf.summary.histogram(name + \"_hist\", hidden)\n",
    "logits = tf.layers.dense(hidden, 10, kernel_initializer=tf.random_uniform_initializer(minval=-w_range, maxval=w_range),\n",
    "                         bias_initializer=tf.zeros_initializer, name=\"logits\")\n",
    "\n",
    "# embedding\n",
    "orig_embedding_vec = tf.Variable([0.], name='orig_embedding')\n",
    "save_orig_embeddings = tf.assign(orig_embedding_vec, imgs, validate_shape=False)\n",
    "\n",
    "embedding_vec = tf.Variable([0.], name='embedding')\n",
    "save_embeddings = tf.assign(embedding_vec, hidden, validate_shape=False)\n",
    "\n",
    "# create the cost and a single training step based on that\n",
    "labels = tf.placeholder(tf.uint8, [None])\n",
    "labels_onehot = tf.one_hot(indices=labels, depth=10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits))\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.03)\n",
    "for g, v in optimizer.compute_gradients(cross_entropy):\n",
    "    if 'kernel' in v.name:\n",
    "        tf.summary.scalar(\"grad_{}\".format(v.name.replace(':','_')), tf.norm(g))\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# evaluation measures\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# train!\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tb_dir, sess.graph)\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = orig_embedding_vec.name\n",
    "    embedding_conf.metadata_path = 'metadata.tsv'\n",
    "    embedding_conf.sprite.image_path = 'sprite.jpg'\n",
    "    embedding_conf.sprite.single_image_dim.extend([28, 28])\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    test_xs, test_ys = mnist.test.images, mnist.test.labels\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(1000)\n",
    "        stats, _ = sess.run([merged, train_step], feed_dict={imgs: batch_xs, labels_onehot: batch_ys})\n",
    "        writer.add_summary(stats, step)\n",
    "        if step % 100 == 0:\n",
    "            train_acc, _ = sess.run([accuracy, cross_entropy], feed_dict={imgs: batch_xs, labels_onehot: batch_ys})\n",
    "            print(\"Training accuracy on the {}th step: {}\".format(step, train_acc))\n",
    "    \n",
    "    sess.run([save_orig_embeddings], feed_dict={imgs: test_xs[:num_to_viz, :],\n",
    "                                           labels_onehot: test_ys[:num_to_viz, :]})\n",
    "    saver.save(sess, os.path.join(tb_dir, \"model.ckpt\"))\n",
    "\n",
    "    # now evaluate\n",
    "    print(\"Final test accuracy: {}\".format(sess.run([accuracy, cross_entropy],\n",
    "                                                    feed_dict={imgs: test_xs, labels_onehot: test_ys})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels as metadata\n",
    "with open(os.path.join(tb_dir, 'metadata.tsv'), 'w') as fout:\n",
    "    for label in list(np.where(test_ys==1)[1][:num_to_viz]):\n",
    "        fout.write('{}\\n'.format(label))         \n",
    "        \n",
    "make_sprite(test_xs[:num_to_viz, :], os.path.join(tb_dir, 'sprite.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Training accuracy on the 0th step: 0.09200000017881393\n",
      "Training accuracy on the 100th step: 0.1810000091791153\n",
      "Training accuracy on the 200th step: 0.27799999713897705\n",
      "Training accuracy on the 300th step: 0.3439999520778656\n",
      "Training accuracy on the 400th step: 0.687000036239624\n",
      "Training accuracy on the 500th step: 0.8159999847412109\n",
      "Training accuracy on the 600th step: 0.859000027179718\n",
      "Training accuracy on the 700th step: 0.9039999842643738\n",
      "Training accuracy on the 800th step: 0.9039999842643738\n",
      "Training accuracy on the 900th step: 0.9080000519752502\n",
      "Final test accuracy: [0.92160016, 0.26218414]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tb_dir = 'summaries/embeddings'\n",
    "\n",
    "# get the data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# define the model first, from input to output\n",
    "imgs = tf.placeholder(tf.float32, shape=[None, 28*28])\n",
    "\n",
    "\n",
    "n_hidden = 100\n",
    "n_layers = 8\n",
    "w_range = 0.1\n",
    "hidden = imgs\n",
    "num_to_viz = 256\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    name = \"hidden_{}\".format(layer)\n",
    "    hidden = tf.layers.dense(hidden, n_hidden, activation=tf.nn.relu,\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(\n",
    "                                 uniform=True),\n",
    "                             bias_initializer=tf.constant_initializer(0.01), name=name)\n",
    "    tf.summary.histogram(name + \"_hist\", hidden)\n",
    "logits = tf.layers.dense(hidden, 10, kernel_initializer=tf.random_uniform_initializer(minval=-w_range, maxval=w_range),\n",
    "                         bias_initializer=tf.zeros_initializer, name=\"logits\")\n",
    "\n",
    "# embedding\n",
    "#orig_embedding_vec = tf.Variable([0.], name='orig_embedding')\n",
    "#save_orig_embeddings = tf.assign(orig_embedding_vec, imgs, validate_shape=False)\n",
    "\n",
    "embedding_vec = tf.Variable([0.], name='embedding')\n",
    "save_embeddings = tf.assign(embedding_vec, hidden, validate_shape=False)\n",
    "\n",
    "# create the cost and a single training step based on that\n",
    "labels = tf.placeholder(tf.uint8, [None])\n",
    "labels_onehot = tf.one_hot(indices=labels, depth=10)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_onehot, logits=logits))\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.03)\n",
    "for g, v in optimizer.compute_gradients(cross_entropy):\n",
    "    if 'kernel' in v.name:\n",
    "        tf.summary.scalar(\"grad_{}\".format(v.name.replace(':','_')), tf.norm(g))\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# evaluation measures\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# train!\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(tb_dir, sess.graph)\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embedding_vec.name\n",
    "    embedding_conf.metadata_path = 'metadata.tsv'\n",
    "    embedding_conf.sprite.image_path = 'sprite.jpg'\n",
    "    embedding_conf.sprite.single_image_dim.extend([28, 28])\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    test_xs, test_ys = mnist.test.images, mnist.test.labels\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(1000)\n",
    "        stats, _ = sess.run([merged, train_step], feed_dict={imgs: batch_xs, labels_onehot: batch_ys})\n",
    "        writer.add_summary(stats, step)\n",
    "        if step % 100 == 0:\n",
    "            train_acc, _ = sess.run([accuracy, cross_entropy], feed_dict={imgs: batch_xs, labels_onehot: batch_ys})\n",
    "            print(\"Training accuracy on the {}th step: {}\".format(step, train_acc))\n",
    "    \n",
    "    sess.run([save_embeddings], feed_dict={imgs: test_xs[:num_to_viz, :],\n",
    "                                           labels_onehot: test_ys[:num_to_viz, :]})\n",
    "    saver.save(sess, os.path.join(tb_dir, \"model.ckpt\"))\n",
    "\n",
    "    # now evaluate\n",
    "    print(\"Final test accuracy: {}\".format(sess.run([accuracy, cross_entropy],\n",
    "                                                    feed_dict={imgs: test_xs, labels_onehot: test_ys})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels as metadata\n",
    "with open(os.path.join(tb_dir, 'metadata.tsv'), 'w') as fout:\n",
    "    for label in list(np.where(test_ys==1)[1][:num_to_viz]):\n",
    "        fout.write('{}\\n'.format(label))         \n",
    "        \n",
    "make_sprite(test_xs[:num_to_viz, :], os.path.join(tb_dir, 'sprite.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
